---
title: "Datamining using RStudio Notebook"
output: 
  html_notebook: 
    highlight: espresso
---

# Data mining & Statistics in R

This notebook is made for students to be able to get more familiar with R, especially when it comes to the common data mining techniques that were previously performed using Python. The exercises within this notebook are in order of the presentation, so feel free to skip to any particular exercise which you want to test out.

# 1. Statistics

In the statistics chapter, we will go over some ways R can be used to perform some statistical analysis.

## 1.1 Data preparation

The first thing we will have to do is loading in the dataset. To load the CSV, simply use the following command:

```{r}
statistic_df <- read.csv('GSS2012.csv')
```

To make sure the data has been loaded in correctly, we can use the head function to show the first couple of entries.

```{r}
head(statistic_df)
```

As you can see, we got our data loaded in, but there are quite a lot of columns. Luckily for us, we can just select the columns we want to work with, and we can even store that in a new variable! let's give this variable an easy to recognize name.\
After that, we can once again see the results by using the head function.

```{r}
seldf <- select(statistic_df, c('life', 'mar1', 'sex'))
head(statistic_df)
```

As you can see, we now just have the three columns left. But even in these first few records we can already see some missing values. We can see which rows have empty values using the following line:

```{r}
which(seldf == "" | is.na(seldf))
```

Those are quite a few missing values! We can remove these, but to ensure we also remove the records with an empty string, we first transform the empty values to NA, after which we can omit those records.

```{r}
seldf[seldf == ""] <- NA
seldf <- na.omit(seldf)
```

To see how many records have been removed, we can compare the amount of total records of our original dataframe with the cleaned dataframe by using the *nrow()* function*.*

```{r}
df_count = nrow(statistic_df)
seldf_count = nrow(seldf)

print(paste0('The amount of rows in the df variable is ', df_count, ' and for the seldf variable it is ', seldf_count, '. This means we have removed ', (df_count - seldf_count), ' records' ))
```

Now that we have removed any missing values, it's time to get to work with our cleaned dataset.\
Let's find out whether a certain lifestyle has any influence on the marital status of a person.\
There is a handy function to create a table with the columns we want to show, so let's use that. We can select the columns from a specific dataframe with the \$ sign as show below:

```{r}
table(seldf$sex, seldf$life)
```

We can use the prop.table() function to transform the numbers above in percentages. By default, the prop.table() function's output adds up to 1. This means that to get to a 100%, we simply multiply the values by 100.

```{r}
prop.table(table(seldf$sex, seldf$life))*100
```

It looks like most people reported having an exciting life. Lets find out if the marital status has any influence on this. And while we're at it, let's also split the data in male and female using the ftable() function.

```{r}
prop.table(ftable(seldf))*100
```

It seems like most people leading an exciting life are married, so let's narrow it down some more.\
Let's see the percentages on how many married women think their life is exciting.

```{r}
females_df <- seldf[seldf$mar1 == "MARRIED" & seldf$sex == 'FEMALE', ]
prop.table(table(females_df$life))*100
```

And let's do the same for men.

```{r}
males <- seldf[seldf$mar1 == "MARRIED" & seldf$sex == 'MALE', ]
prop.table(table(males$life))*100
```

But we can also combine the data using the ftable() function, where we just filter the mar1 column where the value is MARRIED

```{r}
combined_df <- seldf[seldf$mar1 == "MARRIED", ]
ftable(combined_df)
```

And get it in percentages:

```{r}
prop.table(ftable(combined_df))*100
```

## 1.2 Time to get to work

Now it's time to work on the exercises.\
First, let's create a new variable based on the *statistic_df* variable, containing the sex, age and marital status, and show the first couple of rows.

```{r}
# create a variable based on statistic_df and show the first rows
```

Next, let's look for the records containing either an empty string or NA values.

```{r}
# show the data with either an empty string or NA values
```

Let's omit those values, then print the original amount of records, the new amount of records and the difference between those. *(you can make it easier for yourself by storing the result in a variable).*

```{r}
# omit empty strings or NA values, then print the results.
```

Now that the data has been cleaned, let's show it in a table!

```{r}
# show the data in a table.
```

Those are still a lot of records.... Let's see if we can decrease the size by only selecting people with an age range between 25 and 35.

```{r}
# filter the data where the age columns is between 25 and 35
```

Now that we've significantly reduced our sample size, let's see if we can split the data into male and female. You can still keep it in one table for now.

```{r}
# create a table showing the overview
```

Now, select the female records only, then show another table and try to adjust the numbers to percentages.

```{r}
# select female records, then show the values in percentages
```

And do the same for the male records only, but let's switch it up and use absolute numbers.

```{r}
# select male records, then show the values in absolute numbers.
```

And now try it with the combined data:

```{r}
# create a table containing both male and female records.
```

# 2. Regression

In the regression part we will be covering linear regression. Below follows an example as to how you'd perform Single linear regression using the diabetes dataset.

## 2.1 Data preparation

First, we put the dataset in a variable using the read.csv command.

```{r}
diab_DF <- read.csv("diabetes.csv", header=TRUE, sep=",")
head(diab_DF, 10)
```

For this example, we'll be using the glucose levels of a person to estimate their BMI. So let's only select these columns from our dataframe.

```{r}
diab_DF = diab_DF[, c('Glucose', 'BMI')]
head(diab_DF, 10)
```

Following this we'll be checking for any Na values in our data by using the any to see if there are any, and then sum to see how many there are in total.

```{r}
any(is.na(diab_DF))
sum(is.na(diab_DF))
```

Now, as we can see from the head, there is the potential of there being a value of 0. Let's first see how many of those there are in our dataframe.

```{r}
sum(diab_DF$Glucose == '0'| diab_DF$BMI == '0')
```

This is not desired, so we should remove these from our dataframe. We can do this by simply taking a subset of our data, where we disregard any value of 0 like so.

```{r}
diab_DF <- subset(diab_DF, Glucose != '0')
diab_DF <- subset(diab_DF, BMI != '0')
sum(diab_DF$Glucose == '0'| diab_DF$BMI == '0')
head(diab_DF, 10)
```

All 0 values have now been removed from our dataframe. Let's get to regressing.

## 2.2 Installing and loading packages

Before anything, we'll be needing the caret and caTools libraries, so let's install them first. RTools may ask you to restart R prior to installation. This is highly recommended and you can click yes.

```{r}
install.packages("caret")
install.packages("caTools")
```

Now that that is done we can import them.

```{r}
library(caret)
library(caTools)
```

## 2.3 Train, test, split!

Now that that is loaded we can move on to the next step. Since we want reproducability, we'll be setting our seed. (I am setting it to 100.)

```{r}
set.seed(100)
```

After this we will be splitting our data into training and testing data by using the createDataPartition function from the caret library.

```{r}
split_DF <- sample.split(Y=diab_DF$BMI, SplitRatio=0.7)
trainSet <- subset(x=diab_DF, split_DF==TRUE)
testSet <- subset(x=diab_DF, split_DF==FALSE)
```

## 2.4 Linear regression model training and fitting

The next step is to train our model using the lm method and train function from the caret package.

```{r}
linear_regression_model <- train(BMI ~ Glucose, data = trainSet,
                                 method = "lm",
                                 na.action = na.omit,
                                 preProcess=c("scale","center"),
                                 trControl= trainControl(method="none")
)
```

On to training and testing our model.

```{r}
linear_regression_model.training <- predict(linear_regression_model, trainSet)
linear_regression_model.testing <- predict(linear_regression_model, testSet)
```

## 2.5 Results of our hard labour.

Now that we have trained and tested our model, let's see what it looks like.

```{r}
plot(trainSet$BMI, linear_regression_model.training, col="blue")
plot(testSet$BMI, linear_regression_model.testing, col="red")
```

Voila, we have a visual representation of our model. How about some extra statistics.

```{r}
summary(linear_regression_model)

R.training <- cor(trainSet$BMI, linear_regression_model.training)
R.test <- cor(testSet$BMI, linear_regression_model.testing)

R2.training <- R.training^2
R2.test <- R.test^2

print("R train")
print(R.training)
print("R2 train")
print(R2.training)
print("R test")
print(R.test)
print("R2 test")
print(R2.test)
```

## 2.6 Your turn!

Here follow some exercises for you to do!

***Exercise 1.***

```{r}
# Load the Soccer2019C.csv dataset and show the first 5 rows.
```

***Exercise 2.***

```{r}
# We'll be using the "Age", and overall columns, so assign those 2 columns to the dataframe.
```

***Exercise 3.***

```{r}
# Check for Na values, and remove any if they are there.
```

***Exercise 4.***

```{r}
# Install the caret and caTools packages.
```

```{r}
# Import the caret and caTools packages.
```

***Exercise 5.***

```{r}
# Set the seed to 100
```

***Exercise 6.***

```{r}
# Split the dataframe.
```

***Exercise 7.***

```{r}
# Create your model.
```

***Exercise 8.***

```{r}
# Train and test your model.
```

***Exercise 9.***

```{r}
# Plot a graph for both your training and testing data.
```

***Exercise 10.***

```{r}
# Print a summary of the model.
```

***Exercise 11.***

```{r}
# Calculate the correlation coefficient, and print the R's and R2's of training and testing respectively.
```

# 3. Classification

In this part we will cover how you can perform basic classification in R. For this demonstration, we are going to use the UFC2019.csv file as our data-set. And diabetes.csv data-set for the individual exercise where you can put your knowledge to the test.

## 3.1 Data visualization

For data visualization, R provides similar features which were discussed during the DMS course with Python. Those features are plots, bar chart, pie charts, etc. The most basic way to visualize our data, however, is by looking at the first couple of rows and the available columns in the designated data-set. Let's start off by reading out the UFC2019.csv file and displaying the first five rows.

To achieve the following result, we can use the **read.csv()** function and assign it to a variable which will construct a data frame based on the given file, which in our case is a CSV. The parameters of the function are self-explanatory (path of the file, whether to include the header and the separation sign of the data-set file). To display the first few rows of the data-frame, we can use the **head()** method for which we can pass our data-frame and the number of rows we want to be displayed.

```{r}
selection_DF <- read.csv("UFC2019.csv", header=TRUE, sep=",")
head(selection_DF, 5)
```

This data-set consists of 145 unique columns and 5144 unique rows. In order to perform classification on this data-set we should choose at least two independent **X** variables to predict the dependent **y** variable.

To keep it simple, lets choose the **age** of Red and Blue fighters in order to predict the **winner** of the match. To achieve this, lets first take all of the variables that we want to use (both X and y) into a separate data-frame to make it easier for us.

To assign these three columns to a new data-frame, we can use the **[, c(X1, X2, y)]** extension which simply concatenates the columns along with their values to the new data-frame.

```{r}
classification_DF = selection_DF[, c('R_age', 'B_age', 'Winner')]
head(classification_DF, 5)
```

## 3.2 Data cleaning

In order to provide the most accurate results and to avoid any complication with the classifiers, we should clean our data as much as possible. The first thing we can do is check whether our data contains any missing values, such as NaN.

We can use the **any** method in combination with the boolean expression method **is.na()** to simply determine whether our data-set contains missing values. Another alternative is to use the **sum()** in combination with **is.na()** to determine how many missing values we have

```{r}
any(is.na(classification_DF))
sum(is.na(classification_DF))
```

Now that we know that our data-set contains some missing values, let's make sure to get rid of these values to avoid any possible complication with our classifiers. To do this, we can use the **na.omit()** , pass our data-set and assign this to our original data frame variable.

```{r}
classification_DF <- na.omit(classification_DF)
any(is.na(classification_DF))
```

We previously mentioned that we would like to determine whether Red or Blue has won based on the ages of the contestants. In order to make our prediction a bit more accurate and simple, lets remove the possibility of there being a "Draw". In other words, we are only interested in knowing who won or lost. Lets see how many draws we encounter first..

```{r}
sum(classification_DF$Winner == 'Draw')
```

Alright. Lets now remove all the instances of draw to make it more easier on ourselves.

```{r}
classification_DF <- subset(classification_DF, Winner != 'Draw')
sum(classification_DF$Winner == 'Draw')
```

We can not install and import the package that we will need for data partitioning and classification model(s). This package is known as **'caret'**. The caret package offers a unified framework for a wide range of machine learning tasks, including classification, regression, and feature selection. It provides a consistent API for multiple algorithms and facilitates data preprocessing, model tuning, and performance evaluation.

Firstly, lets install the required package.

```{r}
install.packages("caret")
```

Now lets import our packages.

```{r}
library(caret)
```

## 3.3 Data splitting

Now that we have more or less cleaned our data, lets split the data. For this, we should define our independent X and our dependent y variables. To achieve this, we can select which columns we want to assign to the variables either by appointing a range **(classification_DF[, 1:2])** (keep in mind, the range is inclusive, so don't use the index) or the name **(classification_DF\$name_of_column)** of the columns. If we want to pick specific columns that cannot be picked from a range, we can use the **[, c(X1, X3, X5 y)]** way of concatenating columns. However, since we properly split up the data-frame, we should only need to use the first two columns as X and the last as y, which we can do the following way:

```{r}
X <- classification_DF[, 1:2]
y <- classification_DF$Winner
```

Now, let's split our data into train and test sets. To do this, we can use the Hold-Out method which was frequently used in the Data mining and Statistics course. There are several ways of doing it, however, one of the most easiest one to use is the **createDataPartition()** function from the **'caret'** library.

The first parameters indicates the output variable that we are trying to predict. The second parameter indicates what our train and testing split is, in our case, it's 80% train and 20% split. The last parameter indicates that we want to get a vector of indices back, rather than a list of our partitioned data since its easier to work with vectors. This approach will ensure that the data is randomly divided into training and testing sets, allowing us to train our model on one set and evaluate its performance on the other.

```{r}
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]
```

## 3.4 Classification models

There are various libraries that contains various classification models that we can use on our data-set. For simplicity, we are going to use methods and classifier algorithms provided within the **'caret'** library.

**Caret** provides us with many different classification algorithms, some of which are:

1.  Naive Bayes ('naive_bayes')

2.  K-Nearest Neighbors ('knn')

3.  Support Vector Machines ('svmLinear', 'svmRadial')

4.  Random Forest ('rf')

5.  Logistic Regression ('glm')

6.  Quadratic Discriminant Analysis ('qda')

7.  Linear Discriminant Analysis ('lda')

8.  Adaptive Boosting ('adaboost')

Lets try to build our model based on the KNN classifier

```{r}
model <- train(x = X_train, y = y_train, method = "knn")
```

```{r}
predictions <- predict(model, newdata = X_test)
```

## 3.5 Performance assessment

There are several ways to assess the quality of a classification model. The most common is displaying the accuracy of a set of predictions by comparing them with the corresponding true labels **(actual vs predicted) -\> (y_test vs predictions)**

Calculate the accuracy

```{r}
accuracy <- mean(predictions == y_test)
```

Display of the accuracy

```{r}
result <- paste("The accuracy of this model is:", accuracy)
print(result)
```

Another famous method of assessing the quality of a classification model is by construction a confusion matrix which will provide a comprehensive summary of the model's performance by breaking down the predicted and actual class labels into different categories. This way, we can asses the model's accuracy, precision, recall and any other evaluation metrics.

Everything in the top left and bottom right is what the model guessed correctly. The other two corners are the ones that the model was not able to predict correctly. Top left indicates that model predicted Blue right 25 times, while bottom right indicates that the model predicted Red correctly 603 times. Bottom left indicates that 292 times the model predicted Blue to be Red and top right indicates that the model predicted 53 times that Red is Blue.

```{r}
contingency_table <- table(predictions, y_test)
confusionMatrix(contingency_table)
```

## 3.6 Classification exercises

Now that you have a rough idea on how you can perform basic classification, it is your turn! In the cell below you have a list of instructions which you have to follow to perform the exercise. Feel free to use more cells, you will probably need them.

***Exercise 1.***

```{r}
# Import the diabetes.csv file and display the first 5 rows
```

***Exercise 2.***

```{r}
# Check if the data-set contains any missing values and if so, remove them.
```

***Exercise 3.***

```{r}
# Put everything except the 'Outcome' as the X variable and 'Outcome' as the y variable. Tip: the first 8 columns are for the X and 9th is for the y. We are going to try to predict the 'Outcome' based on every other column in the CSV.
```

***Exercise 4.***

```{r}
# Split the data into train and test sets with a 70% train and 30% test ratio
```

***Exercise 5.***

```{r}
# Initialize the model using the Naive Bayes algorithm and feed the data to it (refer to the list of algorithms mentioned in 3.4)
```

***Exercise 6.***

```{r}
# Show the performance of the model using a confusion matrix
```

***Exercise 7.***

```{r}
# Create a a function which takes in the X, y and the name of the model as parameters. Make sure that the data is split into train and test inside the function. Lastly, make sure that the function returns the confusion matrix representing the performance of the model.
```

***Exercise 8.***

```{r}
# Call the previously created function using the Quadratic Discriminant Analysis as your algorithm name (refer to the list of algorithms mentioned in 3.4)
```

# 4. Clustering

## **4.1 What is clustering analysis?**

Cluster analysis is a way to group similar things together based on their characteristics, while keeping the different groups as different as possible from each other. The goal is to find patterns in data by grouping similar observations together.

The two main ways to group data into classes are:

1.  K-means clustering: This is used when you know how many groups you want to create beforehand. It's like putting things in labeled boxes based on their similarity.

2.  Hierarchical clustering: This is used when you don't know how many groups you want to create, and the algorithm helps you determine the best number. It's like starting with a big pile of items and grouping them into smaller and smaller clusters based on their similarities.

K-means is considered supervised because you guide it by telling it how many groups to create. Hierarchical clustering is considered unsupervised because the algorithm determines the number of clusters. There are also many other clustering methods available, in case you want to explore them, they can be found here: <https://easystats.github.io/parameters/articles/clustering.html>

## **4.2 K-means clustering**

K-means clustering, also known as the mobile center algorithm, is a method used for classifying data into k clusters based on the closest average. The goal is to assign each observation to the cluster with the closest prototype. We won't delve into the mathematical specifics, but rather focus on how to implement this technique in R.

First we will need to install some packages and then import them:

```{r}
install.packages("factoextra")
install.packages("NbClust")
install.packages("ggplot2")
install.packages("parameters")
install.packages("mclust")
```

```{r}
library(ggplot2)
library(factoextra)
library(NbClust)
library(parameters)
library(mclust)
```

Then, let's load out dataset

```{r}
# load dataset
dataset <- read.csv("Mall_Customers.csv") 

# show first 5 records
head(dataset,5)
```

The following na.omit is used to remov any rows or columns containing missing values from the dataset. This can be useful for cleaning data before analysis, as many statistical and machine learning algorithms cannot handle missing values and may produce errors or incorrect results if they are present in the data.

```{r}
# remove null values
dataset <- na.omit(dataset)
```

The following command counts the number of missing values in the dataset, if 0 this means we can continue and our dataset is suitable to do the analysis.

```{r}
sum(is.na(dataset))
```

**`dim()`** is a function in R that returns the dimensions of a matrix or data frame, which is similar to df.shape() in python.

```{r}
dim(dataset) 
```

Now we want to cluster our customers based on their income and spendingScore:

```{r}

# Get the two columns of interest
data_2cols <- dataset[, c("Income", "spendingScore")]
head(data_2cols)
```

Visualize our dataset:

```{r}
ggplot(dataset, aes(Income, spendingScore, color = "red")) +
    geom_point(alpha = 0.25) +
    xlab("Income") +
    ylab("Spending Score")
```

K-means clustering is known for its limitation in producing consistent results, as the algorithm randomly selects the initial centers which can result in different classifications even with the same command. To address this issue, the nstart argument in the kmeans() function can be used. By running the algorithm with multiple initialization, the function selects the most optimal one and provides a more stable classification. Looking at the above plot perhaps k=5 is the best value for number of cluster, therefore we will use it like the following:

```{r}
# perform kmeans using k =5
km.out <- kmeans(data_2cols, centers = 5, nstart = 20)
km.out
```

Visualize the results:

```{r}
dataset$cluster_id <- factor(km.out$cluster)
ggplot(dataset, aes(Income, spendingScore, color = cluster_id)) +
    geom_point(alpha = 0.25) +
    xlab("Income") +
    ylab("Spending Score")
```

```{r}
fviz_cluster(km.out, data = data_2cols, 
  ellipse.type = "convex",
  palette = "jco",
  repel = TRUE)
```

```{r}
fviz_cluster(km.out, data = data_2cols,
             palette = "jco", 
             ellipse.type = "euclid", # Concentration ellipse
             star.plot = TRUE, # Add segments from centroids to items
             repel = TRUE, # Avoid label overplotting (slow)
             ggtheme = theme_minimal()
             )
```

Seems reasonable, now lets make sure that 5 is the optimal number of clusters. In order to find the optimal number of clusters for a k-means, it is recommended to choose it based on:

-   Elbow method (which uses the within cluster sums of squares)

-   Average silhouette method

-   Gap statistic method

-   Consensus-based algorithm

now lets start by using the elbow method, the Elbow method looks at the total within-cluster sum of square (WSS) as a function of the number of clusters.

```{r}
# Elbow method
fviz_nbclust(data_2cols,kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2) + # add line for better visualisation
  labs(subtitle = "Elbow method") # add subtitle
```

The location of a knee in the plot is usually considered as an indicator of the appropriate number of clusters because it means that adding another cluster does not improve much better the partition. This method seems to suggest 4 clusters.

The Elbow method is sometimes ambiguous and an alternative is the average silhouette method. The Silhouette method measures the quality of a clustering and determines how well each point lies within its cluster.

```{r}
fviz_nbclust(data_2cols, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
```

The Silhouette method suggests 6 clusters. Let's do the last method which is gap_stat:

```{r}
fviz_nbclust(data_2cols, kmeans,
  nstart = 25,
  method = "gap_stat",
  nboot = 500 # reduce it for lower computation time (but less precise results)
) +
  labs(subtitle = "Gap statistic method")
```

The optimal number of clusters is the one that maximizes the gap statistic. This method suggests only 1 cluster (which is therefore a useless clustering in this case).

As we can see these three methods do not necessarily lead to the same result. Here, the 3 approaches suggest a different number of clusters.

Because no method is clearly better, a fourth alternative is to run many methods and take the number of clusters that is the most agreed upon (i.e., find the consensus).

This can easily be done with the `n_clusters()` function from the `{parameters}` package, this function runs many existing procedures for determining how many clusters are present in your data. It returns the number of clusters based on the maximum consensus. In case of ties, it will select the solution with the less clusters:

```{r}
n_clust <- n_clusters(data_2cols,
  package = c("easystats", "NbClust", "mclust"),
  standardize = FALSE
)
n_clust
```

The optimal number of clusters to retain can also be visualized:

```{r}
plot(n_clust)
```

Based on all indices, most methods suggest to retain 6 clusters, therefore let's do kmeans using k=6.

```{r}
km.out_6 <- kmeans(data_2cols, centers = 6, nstart = 20)
km.out_6
```

```{r}
dataset$cluster_id <- factor(km.out_6$cluster)
ggplot(dataset, aes(Income, spendingScore, color = cluster_id)) +
    geom_point(alpha = 0.25) +
    xlab("Income") +
    ylab("Spending Score")
```

```{r}
fviz_cluster(km.out_6, data = data_2cols, 
  ellipse.type = "convex",
  palette = "jco",
  repel = TRUE)
```

```{r}
fviz_cluster(km.out_6, data = data_2cols,
             palette = "jco", 
             ellipse.type = "euclid", # Concentration ellipse
             star.plot = TRUE, # Add segments from centroids to items
             repel = TRUE, # Avoid label overplotting (slow)
             ggtheme = theme_minimal()
             )
```

## 4.3 Clustering exercises

***Exercise 1.***

Load the Soccer2019C.csv data and then create a scatter plot of "Finishing" and "HeadingAccuracy"

```{r}
# your code here…
```

***Exercise 2.***

How many clusters would you suspect there to be, just by visual inspection?

```{r}
# your code here…
```

***Exercise 3.***

Check if the elbow plots match your suspicion

```{r}
# your code here…
# Because dataset is too large n_cluster will take a lot of time, therefore limit the rows to 1000, not the optimal way but to give an overview over how it works
```

***Exercise 4.***

Create a scatter plot with the number of clusters

```{r}
# your code here…
```

# 5. Recommendations

In this exercise, we will create a user-based collaborative filtering recommendation system using a movie dataset.

## 5.1 Install and Load Required Libraries

```{r}

#install.packages("recommenderlab")
#install.packages("caret")

# Load the recommenderlab library for recommender systems
library(recommenderlab)

# Load the caret library for data splitting
library(caret)

```

## 5.2 Load Ratings Data

```{r}
# Load the ratings data from the CSV file
ratings <- read.csv("ratings.csv")

# see ratings table
head(ratings)

```

```{r}
# Load the movie data from the CSV file
movies <- read.csv("movies.csv")
# see movies table
head(movies)
```

## 5.3 Convert Ratings Data to a realRatingMatrix

In this step, we convert the ratings data to a **`realRatingMatrix`** using the **`as`** function. This step transforms the ratings data into a format suitable for building the recommendation model

```{r}
# Convert ratings data to realRatingMatrix
ratingMatrix <- as(ratings, "realRatingMatrix")

```

## 5.4 Split Data into Training and Testing Sets

Splits the **`ratingMatrix`** data into `training` and `testing` sets based on users. The training set contains 80% of the users selected randomly, and the testing set contains the remaining users.

```{r}
# Split data into training and testing sets
set.seed(123)  # Set seed for reproducibility

# User-based split
user_ids <- colnames(ratingMatrix)
train_users <- sample(user_ids, 0.8 * length(user_ids))  # Select 80% of users randomly


# Create the training and testing data based on the selected users
train <- ratingMatrix[, train_users]
test <- ratingMatrix[, !(user_ids %in% train_users)]
```

## 5.5 Build the Recommendation Model

In this step, we build the recommendation model using the **`Recommender`** function from the **`recommenderlab`** library. We use the "UBCF" method for user-based collaborative filtering.

```{r}
# Build the recommendation model
rec <- Recommender(train, method = "UBCF")

```

## 5.6 Generate Recommendations

Here, we generate recommendations for a specific user (in this case, user 100) by using the **`predict`** function on the recommendation model. We specify **`n = 5`** to get the top 5 recommendations. The recommended movie IDs are stored in the **`recommended_movie_ids`** variable.

```{r}
# Generate recommendations for user 100
user_id <- 100
user_data <- train[user_id, ]
recommendations <- predict(rec, newdata = user_data, n = 5)
recommended_movie_ids <- as(recommendations, "list")

```

## 5.7 Retrieve Movie Details for Recommended Movies

In this step, we retrieve the movie details for the recommended movie IDs from the **`movies`** dataset. We filter the **`movies`** dataset based on the recommended movie IDs and select the "title", "rating", and "num_ratings" columns. The resulting dataframe is stored in the **`recommended_movies`** variable.

```{r}
# Retrieve movie titles for recommended movie IDs
recommended_movie_indices <- match(recommended_movie_ids[[1]], movies$movieId)
recommended_movie_titles <- movies$title[recommended_movie_indices]

# Calculate the number of ratings for each recommended movie from all users
recommended_movie_num_ratings <- as.data.frame(table(ratings$movieId)[as.character(recommended_movie_ids[[1]])])
colnames(recommended_movie_num_ratings) <- c("Movie", "Num_Ratings")

# Calculate the average rating for each recommended movie
recommended_movie_avg_ratings <- aggregate(rating ~ movieId, data = ratings[ratings$movieId %in% recommended_movie_ids[[1]], ], FUN = mean)$rating

#data frame with the recommended movies and their details
recommended_movies_df <- data.frame(
  Movie = recommended_movie_titles,
  Avg_Rating = recommended_movie_avg_ratings,
  Num_Ratings = recommended_movie_num_ratings$Num_Ratings
)

print(recommended_movies_df)

```

## 5.8 Show the results of Recommended Movies

```{r}
# Filter the recommended movies based on the chosen threshold
threshold <- 20
filtered_movies <- recommended_movies_df[recommended_movies_df$Num_Ratings >= threshold, ]
print(filtered_movies)
```

The recommended movie of user 100 will be the **`Full Metal Jacket (1987)`**.

**Recommendations exercise:**

Write a Function to Get Top Movie Recommendations for a User

-   Objective: Create a function that takes a user ID as input and returns the top movie recommendations for that user.

```{r}
# Write a Function to Get Top Movie Recommendations for a User
get_recommendations <- function(user_id) {
  
    #step 1:
            # A. Extract User Data from a training dataset:

            # B. Generate Recommendations:

            # C. Convert Recommendations to a List:

  #step 2:  Retrieve movie titles for recommended movie IDs
  #Step 3:  Calculate the number of ratings for each recommended movie from all                users
  #step 4:  Calculate the average rating for each recommended movie
  #step 5:  Create a data frame with the recommended movies and their details
  
  
}
```

```{r}

# Test the function with your chosen user
user_id <- 50
recommended_movies <- get_recommendations(user_id)
print(recommended_movies)
```

# 6. Answers to the exercises

This section contains the answers to all of the exercises which were given during the workshop. If you did not get to finish the exercises or would like to know what the possible answers could be, feel free to refer to this section.

## 6.1 Solution to the statistics exercises

*create a variable based on statistic_df and show the first rows*

```{r}
mydf <- select(statistic_df, c('sex', 'age', 'mar1'))
head(mydf)
```

*show the data with either an empty string or NA values*

```{r}
which(mydf == "" | is.na(mydf))
```

*omit empty strings or NA values, then print the results.*

```{r}
mydf[mydf == ""] <- NA
mydf <- na.omit(mydf)

df_count = nrow(statistic_df)
mydf_count = nrow(mydf)

print(paste0('The amount of rows in the df variable is ', df_count, ' and for the seldf variable it is ', mydf_count, '. This means we have removed ', (df_count - mydf_count), ' records' ))
```

*show the data in a table.*

```{r}
table(mydf$age, mydf$mar1)
```

*filter the data where the age columns is between 25 and 35*

```{r}
agedf <- mydf[mydf$age > 25 & mydf$age < 36, ]
table(agedf$age, agedf$mar1)
```

*create a table showing the overview*

```{r}
ftable(agedf)
```

*select female records, then show the values in percentages*

```{r}
agedf <- agedf[agedf$sex == 'FEMALE', ]
prop.table(table(agedf$age, agedf$mar1))*100
```

*select male records, then show the values in absolute numbers.*

```{r}
agedf <- agedf[agedf$sex == 'MALE', ]
table(agedf$age, agedf$mar1)
```

*create a table containing both male and female records.*

```{r}
mycombined <- mydf[mydf$mar1 == 'MARRIED', ]
ftable(mycombined)
```

## 6.2 Solution to the regression exercises

***Exercise 1.***

```{r}
soccer_linear_regression_DF <- read.csv("Soccer2019C.csv", header=TRUE, sep=",")
head(soccer_linear_regression_DF, 5)
```

***Exercise 2.***

```{r}
soccer_linear_regression_DF = soccer_linear_regression_DF[, c('Age', 'Overall')]
head(soccer_linear_regression_DF, 5)
```

***Exercise 3.***

```{r}
any(is.na(soccer_linear_regression_DF))
```

***Exercise 4.*** Installing:

```{r}
install.packages("caret")
install.packages("caTools")
```

Loading:

```{r}
library(caret)
library(caTools)
```

***Exercise 5.***

```{r}
set.seed(100)
```

***Exercise 6.*** Spliting the dataframe

```{r}
split_soccer_linear_DF <- sample.split(Y=soccer_linear_regression_DF$Overall, SplitRatio=0.7)
soccer_train_set <- subset(x=soccer_linear_regression_DF, split_soccer_linear_DF==TRUE)
soccer_test_set <- subset(x=soccer_linear_regression_DF, split_soccer_linear_DF==FALSE)
```

***Exercise 7.*** Creating the model

```{r}
linear_regression_soccer_model <- train(Overall ~ Age, data = soccer_train_set,
                                 method = "lm",
                                 na.action = na.omit,
                                 preProcess=c("scale","center"),
                                 trControl= trainControl(method="none")
)
```

***Exercise 8.*** Train and test the model.

```{r}
linear_regression_soccer_model.training <- predict(linear_regression_soccer_model, soccer_train_set)
linear_regression_soccer_model.testing <- predict(linear_regression_soccer_model, soccer_test_set)
```

***Exercise 9.***

```{r}
plot(soccer_train_set$Overall, linear_regression_soccer_model.training, col="blue")
plot(soccer_test_set$Overall, linear_regression_soccer_model.testing, col="red")
```

***Exercise 10.***

```{r}
summary(linear_regression_soccer_model)
```

***Exercise 11.***

```{r}
R.training <- cor(soccer_train_set$Overall, linear_regression_soccer_model.training)
R.test <- cor(soccer_test_set$Overall, linear_regression_soccer_model.testing)

R2.training <- R.training^2
R2.test <- R.test^2

print("R train")
print(R.training)

print("R2 train")
print(R2.training)

print("R test")
print(R.test)

print("R2 test")
print(R2.test)
```

## 6.3 Solution to the classification exercises

```{r}
# Import the Soccer2019C.csv file and display the first 5 rows 
diabetes_DF <- read.csv("diabetes.csv", header=TRUE, sep=",") 
head(diabetes_DF, 5)
```

```{r}
# Check if the data-set contains any missing values and if so, remove them. 
any(is.na(diabetes_DF))
```

```{r}
# Put everything except the 'Outcome' as the X variable and 'Outcome' as the y variable. Tip: the first 8 columns are for the X and 9th is for the y. We are going to try to predict the 'Outcome' based on every other column in the CSV.  
X <- diabetes_DF[, 1:8] 
y <- diabetes_DF$Outcome
```

```{r}
# Split the data into train and test sets with a 70% train and 30% test ratio 
train_index <- createDataPartition(y, p = 0.7, list = FALSE) 
X_train <- X[train_index, ] 
y_train <- y[train_index] 
X_test <- X[-train_index, ] 
y_test <- y[-train_index]
```

```{r}
# Initialize the model using the Naive Bayes algorithm and feed the data to it (refer to the list of algorithms mentioned earlier) 
model <- train(x = X_train, y = y_train, method = "naive_bayes") 
predictions <- predict(model, newdata = X_test)
```

```{r}
# Show the performance of the model using a confusion matrix 
contingency_table <- table(predictions, y_test) 
confusionMatrix(contingency_table)
```

```{r}
# Createa a function which takes in the X, y and the name of the model as parameters. Make sure that the data is split into train and test inside the function. Lastly, make sure that the function returns the confusion matrix representing the performance of the model. 
perform_classification <- function(X, y, model_name) {   
  train_index <- createDataPartition(y, p = 0.8, list = FALSE)   
  X_train <- X[train_index, ]   
  y_train <- y[train_index]   
  X_test <- X[-train_index, ]   
  y_test <- y[-train_index]
  
  model <- train(x = X_train, y = y_train, method = model_name)   
  predictions <- predict(model, newdata = X_test)   
  contingency_table <- table(predictions, y_test)
  
  print(paste("The result of the ", model_name, "model is as followed:"))   
  cat("\n")
  
  confusionMatrix(contingency_table) 
}
```

```{r}
# Call the previously created function using the Quadratic Discriminant Analysis as your algorithm name (refer to the list of algorithms mentioned earlier)
perform_classification(X, y, "qda")
```

## 6.4 Solution to the clustering exercises

```{r}
dataset_soccer <- read.csv("Soccer2019C.csv") 

dataset_soccer <- na.omit(dataset_soccer)

# Get the two columns of interest
data_2cols_soccer <- dataset_soccer[, c("Finishing", "HeadingAccuracy")]
```

```{r}
ggplot(dataset_soccer, aes(Finishing, HeadingAccuracy, color = ID)) +
    geom_point(alpha = 0.25) +
    xlab("Income") +
    ylab("Spending Score")
```

```{r}
n_clust <- n_clusters(data_2cols_soccer[1:1000,],
  package = c("easystats", "NbClust", "mclust"),
  standardize = FALSE
)
n_clust
```

```{r}
plot(n_clust)
```

```{r}
km.out_soccer <- kmeans(data_2cols_soccer, centers = 3, nstart = 20)
km.out_soccer
```

```{r}
dataset_soccer$ID <- factor(km.out_soccer$cluster)
ggplot(dataset_soccer, aes(Finishing, HeadingAccuracy, color = ID)) +
    geom_point(alpha = 0.25) +
    xlab("Finishing") +
    ylab("Heading Accuracy")
```

## 6.5 Solution to the Recommendations exercises

```{r}
# Write a Function to Get Top Movie Recommendations for a User
get_recommendations_answer <- function(user_id) {
  
  #step 1:
          # A. Extract User Data from a training dataset:
user_data <- train[user_id, ]
          # B. Generate Recommendations:
recommendations <- predict(rec, newdata = user_data, n = 5)
          # C. Convert Recommendations to a List:
recommended_movie_ids <- as(recommendations, "list")

  #step 2:  Retrieve movie titles for recommended movie IDs
recommended_movie_indices <- match(recommended_movie_ids[[1]], movies$movieId)
recommended_movie_titles <- movies$title[recommended_movie_indices]

  #Step 3:  Calculate the number of ratings for each recommended movie from all                users.
recommended_movie_num_ratings <- as.data.frame(table(ratings$movieId)[as.character(recommended_movie_ids[[1]])])
colnames(recommended_movie_num_ratings) <- c("Movie", "Num_Ratings")

  #step 4:  Calculate the average rating for each recommended movie
recommended_movie_avg_ratings <- aggregate(rating ~ movieId, data = ratings[ratings$movieId %in% recommended_movie_ids[[1]], ], FUN = mean)$rating

  #step 5:  Create a data frame with the recommended movies and their details
recommended_movies_df <- data.frame(
  Movie = recommended_movie_titles,
  Avg_Rating = recommended_movie_avg_ratings,
  Num_Ratings = recommended_movie_num_ratings$Num_Ratings
)
  
  
}
```
